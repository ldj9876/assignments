# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IUQ-sWILuv0tslIbMavreXX6KqoSj_5A
"""

import nltk
from nltk.tokenize import word_tokenize
import numpy as np
import json
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

with open('./bbc_articles.json') as json_file:
  json_data = json.load(json_file)

themes = list(json_data.keys())
articles = []
for theme in themes:
  articles = articles + json_data[theme]

POS_corpus = np.ndarray([len(articles)],dtype=np.object)
all_tokens = set()

for i, article in enumerate(articles):
  pos_token = nltk.pos_tag(word_tokenize(article))
  POS_corpus[i] = np.array([t[0].lower()+'/'+t[1] for t in pos_token if t[1] in ['NN','NNS','NNP','NNPS','VB','VBD','VBG','VBN','VBP','VBZ']])
  all_tokens = all_tokens.union(set(POS_corpus[i]))


all_tokens = list(all_tokens)
all_tokens.sort()

all_tokens = list(enumerate(all_tokens))

vector_tokens = dict([[t[1],t[0]] for t in all_tokens] )
vectorization = np.vectorize(lambda t: vector_tokens[t])
vector_article = np.ndarray(POS_corpus.shape, dtype=np.object)
for i, tokens in enumerate(POS_corpus):
  vector_article[i] = vectorization(tokens)


TF = np.zeros([len(articles), len(all_tokens)])
for i in range(TF.shape[0]):
  TF[i] = np.bincount(vector_article[i],minlength=TF.shape[1])

N = len(articles)
dF = np.sum(TF>0, axis=0)

TF_IDF = TF*(np.log(N/dF))
Normalized_TF_IDF = TF_IDF / np.linalg.norm(TF_IDF,axis=1).reshape((300,1))

with open('./2017312576_이동준_TFIDF_raw.txt','w',encoding='UTF-8') as f:
  for i in range(len(themes)):
    for j in range(len(json_data[themes[i]])):
      article = articles[j]
      f.write('('+themes[i]+","+str(j+1)+")\n")
      for tf_idf in Normalized_TF_IDF[i*100 + j]:
        f.write(str(tf_idf)[:6]+"\t") # 여기서 반올림?
      f.write("\n\n")

