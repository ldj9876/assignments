# -*- coding: utf-8 -*-
"""2017312576_이동준_NB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wzQuoHrWFIEmBtK0dsZvwqDHzjQ_6QM7
"""

import nltk, json
from nltk.tokenize import word_tokenize
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
from sklearn.naive_bayes import MultinomialNB

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

with open('./bbc_articles_train.json') as json_file:
  train_data = json.load(json_file)
train = train_data['business'] + train_data['tech'] + train_data['politics']
train_labels = (['business'] * 80) + (['tech'] * 80) + (['politics'] * 80) 

with open('./bbc_articles_test.json') as json_file:
  test_data = json.load(json_file)
test = test_data['business'] + test_data['tech'] + test_data['politics']
test_labels = (['business'] * 20) + (['tech'] * 20) + (['politics'] * 20) 

POS_tags = ['NN','NNS','NNP','NNPS','VB','VBD','VBG','VBN','VBP','VBZ']

train_words=[]
total=set()
for sentence in train:
  pos_token = nltk.pos_tag(word_tokenize(sentence))
  words = [t[0].lower() for t in pos_token if t[1] in POS_tags]
  total = total.union(set(words))
  train_words.append(words)

train_tf=[]
for words in train_words:
  train_tf.append([])
  for word in total:
    train_tf[-1].append(words.count(word))

test_words=[]
for sentence in test:
  pos_token = nltk.pos_tag(word_tokenize(sentence))
  words = [t[0].lower() for t in pos_token if t[1] in POS_tags]
  test_words.append(words)

test_tf=[]
for words in test_words:
  test_tf.append([])
  for word in total:
    test_tf[-1].append(words.count(word))

classifier= MultinomialNB(alpha=0.001)
classifier.fit(train_tf,train_labels)
predict = classifier.predict(test_tf)
label = test_labels

conf_mat =  confusion_matrix(label,predict)

acc = ((accuracy_score(label, predict)*100)//0.0001)*0.0001

macro_precision = ((precision_score(label,predict,average='macro')*100)//0.0001)*0.0001
micro_precision = ((precision_score(label,predict,average='micro')*100)//0.0001)*0.0001

macro_recall = ((recall_score(label,predict,average='macro')*100)//0.0001)*0.0001
micro_recall = ((recall_score(label,predict,average='micro')*100)//0.0001)*0.0001

macro_f1 = ((f1_score(label,predict,average='macro')*100)//0.0001)*0.0001
micro_f1 = ((f1_score(label,predict,average='micro')*100)//0.0001)*0.0001



fw = open("./2017312576_이동준_NB.txt",'w',encoding='UTF-8')
fw.write("Confusion matrix\n")
fw.write(str(conf_mat[0][0])+"\t"+str(conf_mat[0][1])+"\t"+str(conf_mat[0][2])+"\n")
fw.write(str(conf_mat[1][0])+"\t"+str(conf_mat[1][1])+"\t"+str(conf_mat[1][2])+"\n")
fw.write(str(conf_mat[2][0])+"\t"+str(conf_mat[2][1])+"\t"+str(conf_mat[2][2])+"\n\n")

fw.write("Accuracy : "+str(acc)[:7]+"%\n\n")
fw.write("Macro averaging precision : "+str(macro_precision)[:7]+"%\n")
fw.write("Micro averaging precision : "+str(micro_precision)[:7]+"%\n\n")
fw.write("Macro averaging recall : "+str(macro_recall)[:7]+"%\n")
fw.write("Micro averaging recall : "+str(micro_recall)[:7]+"%\n\n")
fw.write("Macro averaging f1-score : "+str(macro_f1)[:7]+"%\n")
fw.write("Micro averaging f1-score : "+str(micro_f1)[:7]+"%")

fw.close()



