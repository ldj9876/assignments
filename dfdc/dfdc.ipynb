{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dfdc.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ah3Ggp-KDZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install face_recognition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mymwsU9clFW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Concatenate\n",
        "from tensorflow.keras.layers import Flatten, Dropout, Dense, LeakyReLU, AveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "class model():\n",
        "    def __init__(self,trained = False, input_shape = (299,299,3)):\n",
        "        if not trained:\n",
        "            self.model2 = self.make_model(input_shape)\n",
        "\n",
        "    def make_model(self,input_shape):\n",
        "        print(\"Tlqkf\")\n",
        "        model_input = Input(shape = input_shape)\n",
        "      \n",
        "        x = Conv2D(32,(3,3),strides=2)(model_input)\n",
        "        x = Conv2D(64,(3,3))(x) \n",
        "\n",
        "        x = self.inception_layer(16,32,32,16)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D(pool_size=(2,2),padding='same')(x)\n",
        "\n",
        "        x = self.inception_layer(8,16,16,16)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D(pool_size=(2,2),padding='same')(x)\n",
        "\n",
        "        x = Conv2D(16,(5,5),padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D(pool_size=(2,2),padding='same')(x)\n",
        "        \n",
        "        x = Flatten()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "        x = Dense(32)(x)\n",
        "        x = LeakyReLU(alpha = 0.2)(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        \n",
        "        y = Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "        return  Model(inputs = model_input, outputs = y)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    def inception_layer(self,n1,n2,n3,n4):\n",
        "        def inception(x):\n",
        "            x1 = Conv2D(n1, (1,1), padding='same', activation = 'relu')(x)\n",
        "\n",
        "            x2 = Conv2D(n2, (1,1), padding='same', activation = 'relu')(x)\n",
        "            x2 = Conv2D(n2, (3,3), padding='same', activation = 'relu')(x2)\n",
        "\n",
        "            x3 = Conv2D(n3, (1,1), padding='same', activation = 'relu')(x)\n",
        "            x3 = Conv2D(n3, (5,5), padding='same', activation = 'relu')(x3)\n",
        "\n",
        "            x4 = MaxPooling2D(pool_size=(3,3),strides=(1,1),padding = 'same')(x)\n",
        "            x4 = Conv2D(n4, (1,1), padding='same', activation = 'relu')(x4)\n",
        "            output = Concatenate()([x1,x2,x3,x4])\n",
        "            return output\n",
        "        return inception\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeLX2aG8qCf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageDraw\n",
        "import cv2 as cv\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import face_recognition\n",
        "\n",
        "class Data(keras.utils.Sequence):\n",
        "    def __init__(self,path, shuffle = False, batch_size = 32, num_frame = 10, get_img = False, mode = 'test'):\n",
        "        super(Data,self).__init__()\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_frame = num_frame\n",
        "        self.shuffle = shuffle\n",
        "        self.mode = mode\n",
        "        if get_img:\n",
        "            if mode =='train':\n",
        "                self.read_json(self.path)\n",
        "            else:\n",
        "                self.test(self.path)\n",
        "        else:\n",
        "            self.read_folder(self.path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.labels) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        files = self.data[self.batch_size * index : self.batch_size * (index+1)]\n",
        "        x = self.get_images(files)\n",
        "        y = self.labels[self.batch_size * index : self.batch_size * (index+1)]\n",
        "        return np.array(x), np.array(y)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "      self.indexes = np.arange(len(self.data))\n",
        "\n",
        "      if self.shuffle:\n",
        "        np.random.shuffle(self.indexes)\n",
        "\n",
        "    def test(self,path):\n",
        "        data = glob.glob(path+\"*.mp4\")\n",
        "        labels = ['test' for x in data]\n",
        "        n=0\n",
        "        path = path+\"images/\"\n",
        "        for i in range(len(data)):\n",
        "            print(i)\n",
        "            label = labels[i]\n",
        "            faces = self.extractor(data[i],labels)\n",
        "            for face in faces:\n",
        "                f_name = path + str(n).zfill(4) + label + \".jpg\"\n",
        "                n = n + 1\n",
        "                cv.imwrite(f_name, cv.cvtColor(face,cv.COLOR_RGB2BGR))\n",
        "                \n",
        "    def read_json(self,path):\n",
        "        json = glob.glob(path+\"*.json\")[0]\n",
        "        json = pd.read_json(json)\n",
        "        files = list(json.keys())\n",
        "        data = [path + x for x in files]\n",
        "        labels = list(json.T['label'])\n",
        "        self.data, self.labels = [], []\n",
        "        n = 0\n",
        "        path = path + \"images/\"\n",
        "        if not (os.path.isdir(path[:-1])):\n",
        "            os.makedirs(path[:-1])\n",
        "        for i in range(len(data)):\n",
        "            print(i)\n",
        "            label = labels[i]\n",
        "            faces = self.extractor(data[i],labels)\n",
        "            for face in faces:\n",
        "                f_name = path + str(n).zfill(4) + label + \".jpg\"\n",
        "                n = n + 1\n",
        "                cv.imwrite(f_name, cv.cvtColor(face,cv.COLOR_RGB2BGR))\n",
        "                self.data.append(f_name)\n",
        "                self.labels.append(label)\n",
        "\n",
        "    def get_images(self,files):\n",
        "        x = []\n",
        "        for file in files:\n",
        "            image = keras.preprocessing.image.load_img(path=file)\n",
        "            img = keras.preprocessing.image.img_to_array(image)\n",
        "            x.append(img)\n",
        "        return x\n",
        "\n",
        "    def extractor(self, file, label):\n",
        "        v_cap = cv.VideoCapture(file)\n",
        "        v_len = int(v_cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "        faces = []\n",
        "        sample = np.linspace(0, v_len - 1, self.num_frame).astype(int)\n",
        "\n",
        "        for i in range(v_len):\n",
        "            success = v_cap.grab()\n",
        "            if i in sample:\n",
        "                success, image = v_cap.retrieve()\n",
        "                if not success:\n",
        "                    break\n",
        "                image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "                shape = image.shape\n",
        "                location = face_recognition.face_locations(image)\n",
        "                if location:\n",
        "                    t, r, b, l = location[0]\n",
        "                else:\n",
        "                    continue\n",
        "                t, r, b, l = max(0,t-40), min(shape[1], r+40), min(shape[0],b+40), max(0,l-40)\n",
        "                face = image[t:b, l:r]\n",
        "                face = cv.resize(face,dsize=(299,299))\n",
        "                faces.append(face)\n",
        "        return faces\n",
        "\n",
        "    def read_folder(self,path):\n",
        "        self.data = glob.glob(path+\"*.jpg\")\n",
        "        self.labels = []\n",
        "        size = len(self.data)\n",
        "        if self.mode == 'train':\n",
        "          self.data = self.data[:int(size*0.8)]\n",
        "        elif self.mode == 'validation':\n",
        "          self.data = self.data[int(size*0.8):]\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.data)\n",
        "        for data in self.data:\n",
        "            if 'FAKE' in data:\n",
        "                self.labels.append(1)\n",
        "            else:\n",
        "                self.labels.append(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Cx25Jq2OqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "15031eec-960e-40fb-8350-6bed9f76f041"
      },
      "source": [
        "t_set = Data(path='/content/drive/My Drive/dataset/train_sample_videos/images/',shuffle=True,get_img=False,mode='train')\n",
        "v_set = Data(path='/content/drive/My Drive/dataset/train_sample_videos/images/',shuffle=False,get_img=False,mode='validation')\n",
        "m = model()\n",
        "inception_model = m.model2\n",
        "\n",
        "callback_list = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"/content/drive/My Drive/dataset/Saved_Model2.h5\",\n",
        "        monitor='val_acc',\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "inception_model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['acc'])\n",
        "inception_model.fit_generator(generator = asdf,steps_per_epoch=len(asdf),validation_data=qwert,\n",
        "                epochs = 20,callbacks=callback_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tlqkf\n",
            "Epoch 1/20\n",
            "89/89 [==============================] - 1330s 15s/step - loss: 0.7150 - acc: 0.7275 - val_loss: 0.8911 - val_acc: 0.8011\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 16s 182ms/step - loss: 0.5750 - acc: 0.7535 - val_loss: 0.8383 - val_acc: 0.6875\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 17s 189ms/step - loss: 0.5240 - acc: 0.7746 - val_loss: 0.4902 - val_acc: 0.7955\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 17s 194ms/step - loss: 0.5073 - acc: 0.7767 - val_loss: 0.8434 - val_acc: 0.8338\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 16s 183ms/step - loss: 0.4938 - acc: 0.7844 - val_loss: 1.4277 - val_acc: 0.6065\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 16s 181ms/step - loss: 0.4982 - acc: 0.7960 - val_loss: 0.7325 - val_acc: 0.8224\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 16s 182ms/step - loss: 0.4284 - acc: 0.8072 - val_loss: 0.6498 - val_acc: 0.7102\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 16s 185ms/step - loss: 0.4088 - acc: 0.8185 - val_loss: 0.5997 - val_acc: 0.6932\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 16s 179ms/step - loss: 0.3735 - acc: 0.8452 - val_loss: 0.4982 - val_acc: 0.8338\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 16s 177ms/step - loss: 0.3593 - acc: 0.8459 - val_loss: 1.6011 - val_acc: 0.5298\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 16s 179ms/step - loss: 0.3324 - acc: 0.8624 - val_loss: 0.4819 - val_acc: 0.8111\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 17s 191ms/step - loss: 0.2965 - acc: 0.8711 - val_loss: 0.3486 - val_acc: 0.8452\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 16s 179ms/step - loss: 0.3076 - acc: 0.8701 - val_loss: 0.5481 - val_acc: 0.8239\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 16s 177ms/step - loss: 0.2843 - acc: 0.8869 - val_loss: 0.6588 - val_acc: 0.8295\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 16s 177ms/step - loss: 0.2543 - acc: 0.8978 - val_loss: 0.6606 - val_acc: 0.7756\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 16s 176ms/step - loss: 0.2181 - acc: 0.9038 - val_loss: 0.5482 - val_acc: 0.8366\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 16s 176ms/step - loss: 0.2306 - acc: 0.9077 - val_loss: 1.3238 - val_acc: 0.6790\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 16s 179ms/step - loss: 0.1890 - acc: 0.9199 - val_loss: 0.8245 - val_acc: 0.7955\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 15s 174ms/step - loss: 0.1855 - acc: 0.9224 - val_loss: 0.7123 - val_acc: 0.8395\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 15s 174ms/step - loss: 0.1793 - acc: 0.9266 - val_loss: 0.7581 - val_acc: 0.8210\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd3cc13eba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31qqGQXXohLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indeption_model.load_weights(\"/content/drive/My Drive/dataset/Saved_Model2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}